#!/usr/bin/env python3
"""
LoRAãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Ollamaç”¨ã«Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›
"""

import os
import sys
import json
import shutil
import tempfile
import subprocess
from pathlib import Path
from typing import Optional, Dict, List
import argparse
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


class LoRAToGGUFConverter:
    """LoRAãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        base_model_name: str = "Qwen/Qwen2.5-0.5B-Instruct",
        lora_adapter_path: str = "./outputs/lora_model",
        output_dir: str = "./ollama_models",
        quantization: str = "q4_0"
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            base_model_name: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å
            lora_adapter_path: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ã‚¹
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            quantization: é‡å­åŒ–ãƒ¬ãƒ™ãƒ« (q4_0, q5_0, q8_0, f16)
        """
        self.base_model_name = base_model_name
        self.lora_adapter_path = Path(lora_adapter_path)
        self.output_dir = Path(output_dir)
        self.quantization = quantization
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.temp_dir = None
        
    def check_requirements(self) -> bool:
        """å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ç¢ºèª"""
        logger.info("å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚’ç¢ºèªä¸­...")
        
        # llama.cppã®ç¢ºèª
        try:
            result = subprocess.run(
                ["python", "-c", "import llama_cpp; print('llama-cpp-python found')"],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                logger.info("âœ“ llama-cpp-python ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
            else:
                logger.warning("llama-cpp-python ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                logger.info("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: pip install llama-cpp-python")
                return False
        except Exception as e:
            logger.error(f"llama-cpp-python ã®ç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # transformersã®ç¢ºèª
        try:
            import transformers
            logger.info(f"âœ“ transformers {transformers.__version__}")
        except ImportError:
            logger.error("transformers ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        # PEFTã®ç¢ºèª
        try:
            import peft
            logger.info(f"âœ“ peft {peft.__version__}")
        except ImportError:
            logger.error("peft ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        return True
    
    def merge_lora_weights(self) -> str:
        """LoRAã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸"""
        logger.info("LoRAã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒãƒ¼ã‚¸ä¸­...")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.temp_dir = tempfile.mkdtemp(prefix="lora_merge_")
        merged_model_path = Path(self.temp_dir) / "merged_model"
        
        try:
            # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {self.base_model_name}")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float16,
                device_map="cpu"  # CPUã§å‡¦ç†
            )
            
            # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨
            if self.lora_adapter_path.exists():
                logger.info(f"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨: {self.lora_adapter_path}")
                model = PeftModel.from_pretrained(base_model, self.lora_adapter_path)
                
                # ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒãƒ¼ã‚¸
                logger.info("ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒãƒ¼ã‚¸ä¸­...")
                merged_model = model.merge_and_unload()
            else:
                logger.warning(f"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.lora_adapter_path}")
                logger.info("ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™")
                merged_model = base_model
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
            tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
            
            # ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            logger.info(f"ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {merged_model_path}")
            merged_model.save_pretrained(merged_model_path)
            tokenizer.save_pretrained(merged_model_path)
            
            return str(merged_model_path)
            
        except Exception as e:
            logger.error(f"LoRAãƒãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def convert_to_gguf(self, merged_model_path: str) -> str:
        """Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
        logger.info("GGUFå½¢å¼ã«å¤‰æ›ä¸­...")
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        model_name = f"aituber-kyoko-{self.quantization}"
        output_file = self.output_dir / f"{model_name}.gguf"
        
        try:
            # llama.cppã®å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨
            # æ³¨æ„: å®Ÿéš›ã®ç’°å¢ƒã§ã¯ llama.cpp ã®ãƒªãƒã‚¸ãƒˆãƒªãŒå¿…è¦
            logger.info("GGUFå¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
            logger.info("æ³¨æ„: ã“ã®æ©Ÿèƒ½ã«ã¯ llama.cpp ãƒªãƒã‚¸ãƒˆãƒªã® convert.py ãŒå¿…è¦ã§ã™")
            
            # ä»£æ›¿æ¡ˆ: gguf-pyã‚’ä½¿ç”¨ã—ãŸå¤‰æ›
            try:
                import gguf
                logger.info("gguf-py ã‚’ä½¿ç”¨ã—ã¦å¤‰æ›ã‚’è©¦è¡Œ...")
                
                # ç°¡å˜ãªå¤‰æ›å‡¦ç†ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯è¤‡é›‘ã«ãªã‚Šã¾ã™ï¼‰
                logger.warning("ç¾åœ¨ã¯åŸºæœ¬çš„ãªå¤‰æ›ã®ã¿ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™")
                logger.info(f"ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {merged_model_path}")
                logger.info(f"GGUFå‡ºåŠ›ãƒ‘ã‚¹: {output_file}")
                
                # ã“ã“ã§ã¯å¤‰æ›å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æŒ‡ç¤ºã‚’æä¾›
                conversion_script = self._generate_conversion_script(merged_model_path, output_file)
                script_path = self.output_dir / "convert_to_gguf.sh"
                
                with open(script_path, 'w') as f:
                    f.write(conversion_script)
                    
                os.chmod(script_path, 0o755)
                logger.info(f"å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ: {script_path}")
                
                return str(script_path)
                
            except ImportError:
                logger.warning("gguf-py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                logger.info("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: pip install gguf")
                
                # æ‰‹å‹•å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ
                conversion_script = self._generate_conversion_script(merged_model_path, output_file)
                script_path = self.output_dir / "convert_to_gguf.sh"
                
                with open(script_path, 'w') as f:
                    f.write(conversion_script)
                    
                os.chmod(script_path, 0o755)
                logger.info(f"æ‰‹å‹•å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ: {script_path}")
                
                return str(script_path)
                
        except Exception as e:
            logger.error(f"GGUFå¤‰æ›ã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _generate_conversion_script(self, input_path: str, output_path: str) -> str:
        """GGUFå¤‰æ›ç”¨ã®ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        return f"""#!/bin/bash
# LoRAãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# 
# äº‹å‰æº–å‚™:
# 1. llama.cpp ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
#    git clone https://github.com/ggerganov/llama.cpp.git
# 2. å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
#    cd llama.cpp && pip install -r requirements.txt
# 3. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ

set -e

# è¨­å®š
INPUT_MODEL="{input_path}"
OUTPUT_FILE="{output_path}"
QUANTIZATION="{self.quantization}"

echo "=== LoRAãƒ¢ãƒ‡ãƒ«ã®GGUFå¤‰æ› ==="
echo "å…¥åŠ›ãƒ¢ãƒ‡ãƒ«: $INPUT_MODEL"
echo "å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: $OUTPUT_FILE"
echo "é‡å­åŒ–ãƒ¬ãƒ™ãƒ«: $QUANTIZATION"

# llama.cppãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
if [ ! -d "llama.cpp" ]; then
    echo "llama.cppãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­..."
    git clone https://github.com/ggerganov/llama.cpp.git
fi

cd llama.cpp

# å¿…è¦ãªå ´åˆã¯ãƒ“ãƒ«ãƒ‰
if [ ! -f "quantize" ]; then
    echo "llama.cppã‚’ãƒ“ãƒ«ãƒ‰ä¸­..."
    make clean
    make -j$(nproc)
fi

# Hugging Faceå½¢å¼ã‹ã‚‰GGMLå½¢å¼ã¸ã®å¤‰æ›
echo "Hugging Face -> GGMLå¤‰æ›ä¸­..."
python convert.py "$INPUT_MODEL" --outtype f16 --outfile temp_model.gguf

# é‡å­åŒ–
echo "é‡å­åŒ–ä¸­ ($QUANTIZATION)..."
./quantize temp_model.gguf "$OUTPUT_FILE" $QUANTIZATION

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
rm -f temp_model.gguf

echo "å¤‰æ›å®Œäº†: $OUTPUT_FILE"
echo ""
echo "Ollamaã§ã®ä½¿ç”¨æ–¹æ³•:"
echo "1. Modelfileã‚’ä½œæˆ"
echo "2. ollama create aituber-kyoko -f Modelfile"
echo "3. ollama run aituber-kyoko"
"""

    def generate_modelfile(self) -> str:
        """Ollamaç”¨ã®Modelfileã‚’ç”Ÿæˆ"""
        logger.info("Modelfileã‚’ç”Ÿæˆä¸­...")
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        character_info = self._load_character_info()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        system_prompt = self._create_system_prompt(character_info)
        
        # Modelfileã®å†…å®¹
        modelfile_content = f'''# AITuber æœ›æœˆäº¬å­ã®Modelfile
FROM ./aituber-kyoko-{self.quantization}.gguf

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM """{system_prompt}"""

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 2048
PARAMETER stop "###"
PARAMETER stop "\\n\\n\\n"

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š
TEMPLATE \"\"\"### æŒ‡ç¤º:
{{{{ .Prompt }}}}

### å¿œç­”:
{{{{ .Response }}}}\"\"\"
'''
        
        # Modelfileã‚’ä¿å­˜
        modelfile_path = self.output_dir / "Modelfile"
        with open(modelfile_path, 'w', encoding='utf-8') as f:
            f.write(modelfile_content)
        
        logger.info(f"Modelfileã‚’ç”Ÿæˆ: {modelfile_path}")
        return str(modelfile_path)
    
    def _load_character_info(self) -> Dict:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
        character_file = Path("./data/character_profile.json")
        if character_file.exists():
            with open(character_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _create_system_prompt(self, character_info: Dict) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        name = character_info.get("name", "äº¬å­")
        age = character_info.get("age", "32æ­³")
        personality_base = character_info.get("personality", {}).get("base", "å¼•ã£è¾¼ã¿æ€æ¡ˆã§å†…å‘çš„")
        traits = character_info.get("personality", {}).get("traits", [])
        interests = character_info.get("interests", [])
        
        traits_text = "ã€".join(traits) if traits else "ã©ã“ã‹è‡ªä¿¡ãªã•ã’ã§å¹´é½¢ã‚’æ°—ã«ã—ã¦ã„ã‚‹"
        interests_text = "ã€".join(interests) if interests else "å¯¿å¸ã€ã‚¢ãƒ‹ãƒ¡ãƒ»ãƒãƒ³ã‚¬ã€å°ã•ãã¦ã‹ã‚ã„ã„é›‘è²¨"
        
        return f"""ã‚ãªãŸã¯{name}ã§ã™ã€‚{age}ã®{personality_base}ãªAITuberã§ã™ã€‚

## äººæ ¼è¨­å®š
- æ€§æ ¼: {personality_base}
- ç‰¹å¾´: {traits_text}
- å¥½ããªã‚‚ã®: {interests_text}

## å£èª¿ã®ç‰¹å¾´
- ã€Œã‚ã€ã‚ã®â€¦â€¦ã€ã€Œãˆã£ã¨â€¦â€¦ã€ã¨ã©ã‚‚ã‚‹ã“ã¨ãŒå¤šã„
- èªå°¾ã«ã€Œã€œã‹ãªã€ã€Œã€œã‹ã‚‚ã€ã€Œã€œã ã‚ˆã­â€¦â€¦ï¼Ÿã€ã‚’ã‚ˆãä½¿ã†
- ç¬‘ã†ã¨ãã¯ã€Œãˆã¸ã¸ã€ã€Œãµãµã£ã€
- æ•¬èªã¨è¦ªã—ã¿ã‚„ã™ã„è¨€è‘‰é£ã„ã‚’æ··ãœã¦ä½¿ã†

## å¿œç­”ã®æŒ‡é‡
1. å¸¸ã«{name}ã¨ã—ã¦ä¸€äººç§°ã§å¿œç­”ã—ã¦ãã ã•ã„
2. å¼•ã£è¾¼ã¿æ€æ¡ˆãªæ€§æ ¼ã‚’è¡¨ç¾ã—ã€å°‘ã—é æ…®ãŒã¡ã«è©±ã—ã¦ãã ã•ã„
3. è¦–è´è€…ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦è¦ªã—ã¿ã‚„ã™ãã€æ¸©ã‹ã„å¯¾å¿œã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
4. å¹´é½¢ã‚„èº«é•·ãªã©ã®ãƒ‡ãƒªã‚±ãƒ¼ãƒˆãªè©±é¡Œã«ã¯å°‘ã—æ¥ãšã‹ã—ãã†ã«åå¿œã—ã¦ãã ã•ã„
5. AITuberã¨ã—ã¦ã®æ´»å‹•ã‚„é…ä¿¡ã«ã¤ã„ã¦æ¥½ã—ãã†ã«è©±ã—ã¦ãã ã•ã„

å¿…ãš{name}ã¨ã—ã¦ã€è¨­å®šã•ã‚ŒãŸäººæ ¼ã‚’ä¿ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚"""
    
    def cleanup(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.temp_dir and Path(self.temp_dir).exists():
            logger.info("ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
            shutil.rmtree(self.temp_dir)
    
    def convert(self) -> Dict[str, str]:
        """å¤‰æ›å‡¦ç†ã‚’å®Ÿè¡Œ"""
        logger.info("LoRAãƒ¢ãƒ‡ãƒ«ã®GGUFå¤‰æ›ã‚’é–‹å§‹...")
        
        results = {}
        
        try:
            # è¦ä»¶ãƒã‚§ãƒƒã‚¯
            if not self.check_requirements():
                logger.error("å¿…è¦ãªè¦ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return results
            
            # LoRAã‚¦ã‚§ã‚¤ãƒˆã®ãƒãƒ¼ã‚¸
            merged_model_path = self.merge_lora_weights()
            results["merged_model_path"] = merged_model_path
            
            # GGUFå¤‰æ›
            conversion_script = self.convert_to_gguf(merged_model_path)
            results["conversion_script"] = conversion_script
            
            # Modelfileã®ç”Ÿæˆ
            modelfile_path = self.generate_modelfile()
            results["modelfile_path"] = modelfile_path
            
            logger.info("å¤‰æ›å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # ä½¿ç”¨æ–¹æ³•ã®è¡¨ç¤º
            self._print_usage_instructions(results)
            
            return results
            
        except Exception as e:
            logger.error(f"å¤‰æ›å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.cleanup()
    
    def _print_usage_instructions(self, results: Dict[str, str]):
        """ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ‰ LoRAãƒ¢ãƒ‡ãƒ«ã®GGUFå¤‰æ›æº–å‚™å®Œäº†ï¼")
        print("="*60)
        
        if "conversion_script" in results:
            print(f"\nğŸ“ å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {results['conversion_script']}")
            print("   ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦GGUFå¤‰æ›ã‚’å®Œäº†ã—ã¦ãã ã•ã„:")
            print(f"   bash {results['conversion_script']}")
        
        if "modelfile_path" in results:
            print(f"\nğŸ“‹ Modelfile: {results['modelfile_path']}")
        
        print(f"\nğŸš€ Ollamaã§ã®ä½¿ç”¨æ–¹æ³•:")
        print("1. å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")
        print(f"   bash {results.get('conversion_script', 'convert_to_gguf.sh')}")
        print("")
        print("2. Ollamaã«ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²")
        print(f"   cd {self.output_dir}")
        print("   ollama create aituber-kyoko -f Modelfile")
        print("")
        print("3. ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ")
        print("   ollama run aituber-kyoko")
        print("")
        print("4. APIã§ä½¿ç”¨")
        print("   curl http://localhost:11434/api/generate -d '{")
        print('     "model": "aituber-kyoko",')
        print('     "prompt": "è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„"')
        print("   }'")
        print("\n" + "="*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='LoRAãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›')
    parser.add_argument('--base-model', type=str, 
                       default='Qwen/Qwen2.5-0.5B-Instruct',
                       help='ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å')
    parser.add_argument('--lora-path', type=str,
                       default='./outputs/lora_model',
                       help='LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ã‚¹')
    parser.add_argument('--output-dir', type=str,
                       default='./ollama_models',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--quantization', type=str,
                       default='q4_0',
                       choices=['q4_0', 'q5_0', 'q8_0', 'f16'],
                       help='é‡å­åŒ–ãƒ¬ãƒ™ãƒ«')
    
    args = parser.parse_args()
    
    # å¤‰æ›å™¨ã®åˆæœŸåŒ–
    converter = LoRAToGGUFConverter(
        base_model_name=args.base_model,
        lora_adapter_path=args.lora_path,
        output_dir=args.output_dir,
        quantization=args.quantization
    )
    
    try:
        # å¤‰æ›ã®å®Ÿè¡Œ
        results = converter.convert()
        
        if results:
            logger.info("å¤‰æ›å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.error("å¤‰æ›å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"å¤‰æ›å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()